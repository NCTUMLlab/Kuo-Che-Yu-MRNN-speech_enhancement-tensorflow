{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow        as tf\n",
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from data_process        import *\n",
    "from Model               import *\n",
    "# from sklearn.manifold    import TSNE\n",
    "\n",
    "NUM_SPEAKER    = 77\n",
    "BATCH_START    = 0\n",
    "BATCH_SIZE     = 30\n",
    "TIME_STEP      = 20 \n",
    "LR             = 0.001\n",
    "lr = 0.01\n",
    "lr_decay = 0.9\n",
    "KEEP_PROB      = 1.0\n",
    "EPOCH_UP       = 50\n",
    "EPOCH_LOW      = 20\n",
    "SPEAKERIDX     = 0\n",
    "INPUT_SIZE     = 513\n",
    "OUTPUT_SIZE    = 513\n",
    "HIDDEN_SIZE    = 1000\n",
    "K              = 2\n",
    "TAO0           = 5.0\n",
    "ANNEAL_RATE    = 0.1\n",
    "MIN_TAU        = 0.5\n",
    "\n",
    "ARCHITECTURE   = {'l0':{'type':'input', 'neurons':INPUT_SIZE}, \n",
    "                  'l1':{'type':'fc', 'neurons':1000}, \n",
    "                  'l2':{'type':'mrnn', 'neurons':HIDDEN_SIZE, 'state_size':K}, \n",
    "                  'l3':{'type':'fc', 'neurons':1000}, \n",
    "                  'l4':{'type':'output', 'neurons':OUTPUT_SIZE}}\n",
    "\n",
    "# ARCHITECTURE   = {'l0':{'type':'input', 'neurons':INPUT_SIZE}, \n",
    "#                   'l1':{'type':'fc', 'neurons':1000}, \n",
    "#                   'l2':{'type':'lstm', 'neurons':HIDDEN_SIZE}, \n",
    "#                   'l3':{'type':'lstm', 'neurons':HIDDEN_SIZE}, \n",
    "#                   'l4':{'type':'fc', 'neurons':700}, \n",
    "#                   'l5':{'type':'output', 'neurons':OUTPUT_SIZE}}\n",
    "\n",
    "TOTAL_SIZE     = 0\n",
    "\n",
    "data           = 'data_merge'\n",
    "SNR            = 5\n",
    "URL            = '../'+data+'/SNR='+str(SNR)+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "mix_train, target1, target2, sequence_length_train = get_data_train(\n",
    "    URL+'Train/Mix/', URL+'Train/Target1/', URL+'Train/Target2/', NUM_SPEAKER, BATCH_SIZE, TIME_STEP, longest=200)\n",
    "end = time.time()\n",
    "print('Get data done!', \"    time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# np.save(URL+'Train/mix', mix_train)\n",
    "# np.save(URL+'Train/t1', target1)\n",
    "# np.save(URL+'Train/t2', target2)\n",
    "# np.save(URL+'Train/sl', sequence_length)\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# mix = np.load(URL+'Train/mix.npy')\n",
    "# t1 = np.load(URL+'Train/t1.npy')\n",
    "# t2 = np.load(URL+'Train/t2.npy')\n",
    "# sl = np.load(URL+'Train/sl')\n",
    "\n",
    "# mix_train=mix_train.item()\n",
    "# target1=target1.item()\n",
    "# target2=target2.item()\n",
    "# sequence_length=sl.item()\n",
    "\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Model(ARCHITECTURE, INPUT_SIZE, OUTPUT_SIZE, BATCH_SIZE, TIME_STEP, LR, activation_function=tf.nn.relu, batch_norm=True)\n",
    "init  = tf.global_variables_initializer()\n",
    "sess  = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MSE = [np.inf] #np.zeros(EPOCH_UP)\n",
    "total_cost = 0\n",
    "model.assign_lr(sess, lr)\n",
    "for ep in range(EPOCH_UP):\n",
    "    # Training\n",
    "    sp_list = [i for i in range(NUM_SPEAKER)]\n",
    "    random.shuffle(sp_list)\n",
    "    sp_idx = 0\n",
    "    BATCH_idx = 0 # to record starting point of batch we want to extract\n",
    "    TIME_idx  = 0\n",
    "    start = time.time()\n",
    "    np_temp=np.maximum(TAO0*np.exp(-ANNEAL_RATE*ep),MIN_TAU)\n",
    "    model.assign_tau(sess, np_temp)\n",
    "    temp_lr, temp_tau = sess.run([model.lr, model.tau])\n",
    "    \n",
    "    while True:\n",
    "        mix, t1, t2, sequence, sp_idx, BATCH_idx, TIME_idx = get_batch_train(\n",
    "            mix_train, target1, target2, BATCH_idx, TIME_idx, sp_idx, BATCH_SIZE, TIME_STEP, INPUT_SIZE, sp_list, \n",
    "            sequence_length_train, longest=200)     \n",
    "        \n",
    "        # break\n",
    "        if sp_idx == NUM_SPEAKER:\n",
    "            break\n",
    "\n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2,\n",
    "                model.keep_prob : KEEP_PROB,\n",
    "                model.training : True\n",
    "            }\n",
    "        \n",
    "        model.sequence_length = sequence\n",
    "        # stochastic gradient descent\n",
    "        _, cost = sess.run(\n",
    "            [model.train_op, model.cost],\n",
    "            feed_dict\n",
    "            )\n",
    "\n",
    "        if math.isnan(cost):\n",
    "            break\n",
    "        model.init_state_assign()\n",
    "        \n",
    "        # cost\n",
    "        total_cost = total_cost + cost\n",
    "    end = time.time()  \n",
    "    train_time = end-start\n",
    "    \n",
    "    TOTAL_SIZE = 0\n",
    "    for key in sequence_length_train.keys():\n",
    "        TOTAL_SIZE += sum(sequence_length_train[key])\n",
    "        \n",
    "    MSE.append(total_cost/(TOTAL_SIZE))\n",
    "    total_cost = 0\n",
    "    \n",
    "    print 'Epoch: ', ep, ' Training Loss: ', MSE[-1], ' Training Time: ', train_time, ' Tau: ', temp_tau, ' LR: ', temp_lr\n",
    "    \n",
    "    model.assign_lr(sess, temp_lr * lr_decay)\n",
    "    if MSE[-2] - MSE[-1] < 0 and ep > 20: #or ep_idx >= 3:\n",
    "        break\n",
    "    #if ep >= EPOCH_LOW and MSE[ep-1]-MSE[ep] < 0.001:\n",
    "    #    MSE = MSE[0:ep+1]\n",
    "    #    break;    \n",
    "        \n",
    "                        \n",
    "print('----------End----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_para = 0\n",
    "for var in tf.trainable_variables():\n",
    "    shape = var.get_shape()\n",
    "    tmp = 1\n",
    "    for dim in shape:\n",
    "        tmp *= dim.value\n",
    "    total_para += tmp\n",
    "print('Total parameters: ', total_para)\n",
    "print(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.plot(MSE)\n",
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "for idx in MSE:\n",
    "    print idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seen speaker test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test1, t1_test1, t2_test1, order, sequence_length = get_data_test(URL+'Test1/Mix/', URL+'Test1/Target1/', URL+'Test1/Target2/', \n",
    "                                                                      NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "sp_list     = mix_test1.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test1[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test1[sp_list[speaker]], t1_test1[sp_list[speaker]], t2_test1[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2,\n",
    "                model.keep_prob : 1.0,\n",
    "                model.training : False\n",
    "            }\n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            cost, pred1, pred2 = sess.run(\n",
    "                [model.cost, model.pred1, model.pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.init_state_assign()\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            cost, pred1, pred2 = sess.run(\n",
    "                [model.cost, model.pred1, model.pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.init_state_assign()\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        # cost\n",
    "        total_cost = total_cost + cost\n",
    "            \n",
    "total_cost = total_cost/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_cost\n",
    "MSE = np.concatenate((MSE, [total_cost]), axis=0)\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "            \n",
    "print('----------End----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('../Result/'+data+'/SNR='+str(SNR)+'/Test1/LSTM'+'_d='+str(HIDDEN_SIZE)+\"_sp_\"+str(NUM_SPEAKER)+'_prediction1.csv', total_pred1, delimiter=\",\")\n",
    "# np.savetxt('../Result/'+data+'/SNR='+str(SNR)+'/Test1/LSTM'+'_d='+str(HIDDEN_SIZE)+\"_sp_\"+str(NUM_SPEAKER)+'_prediction2.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('../Result/'+data+'/SNR='+str(SNR)+'/Test1/MRNN'+'_d='+str(HIDDEN_SIZE)+'_K='+str(K)+\"_sp_\"+str(NUM_SPEAKER)+'_prediction1.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('../Result/'+data+'/SNR='+str(SNR)+'/Test1/MRNN'+'_d='+str(HIDDEN_SIZE)+'_K='+str(K)+\"_sp_\"+str(NUM_SPEAKER)+'_prediction2.csv', total_pred1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unseen speaker test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SPEAKER_TEST = 6\n",
    "mix_test2, t1_test2, t2_test2, order, sequence_length = get_data_test(URL+'Test2/Mix/', URL+'Test2/Target1/', URL+'Test2/Target2/', \n",
    "                                                                      NUM_SPEAKER_TEST, BATCH_SIZE, TIME_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cost  = 0\n",
    "TOTAL_SIZE  = 0\n",
    "total_pred1 = np.zeros((1, 513))\n",
    "total_pred2 = np.zeros((1, 513))\n",
    "sp_list     = mix_test2.keys()\n",
    "sp_list.sort()\n",
    "for speaker in range(NUM_SPEAKER_TEST):\n",
    "    BATCH_START = 0 # to record starting point of batch we want to extract\n",
    "    DATA_SIZE   = mix_test2[sp_list[speaker]].shape[0]\n",
    "    TOTAL_SIZE  = TOTAL_SIZE + DATA_SIZE\n",
    "    for idx in range((DATA_SIZE/(BATCH_SIZE*TIME_STEP))):\n",
    "        BATCH_START = idx*BATCH_SIZE*TIME_STEP\n",
    "        mix, t1, t2 = get_batch_test(\n",
    "            mix_test2[sp_list[speaker]], t1_test2[sp_list[speaker]], t2_test2[sp_list[speaker]], \n",
    "            BATCH_START, BATCH_SIZE, TIME_STEP, INPUT_SIZE, dim=True)\n",
    "        \n",
    "        feed_dict = {\n",
    "                model.x  : mix,\n",
    "                model.y1 : t1,\n",
    "                model.y2 : t2,\n",
    "                model.keep_prob : 1.0,\n",
    "                model.training : False\n",
    "            }\n",
    "        \n",
    "        if idx==(DATA_SIZE/(BATCH_SIZE*TIME_STEP))-1:\n",
    "            model.sequence_length = sequence_length[sp_list[speaker]]\n",
    "            # stochastic gradient descent\n",
    "            cost, pred1, pred2 = sess.run(\n",
    "                [model.cost, model.pred1, model.pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            \n",
    "            model.init_state_assign()\n",
    "            model.sequence_length = [TIME_STEP for i in xrange(0, BATCH_SIZE)]\n",
    "        else:\n",
    "            # stochastic gradient descent\n",
    "            cost, pred1, pred2 = sess.run(\n",
    "                [model.cost, model.pred1, model.pred2],\n",
    "                feed_dict\n",
    "                )\n",
    "            model.init_state_assign()\n",
    "        \n",
    "        total_pred1 = np.append(total_pred1, pred1, axis=0)\n",
    "        total_pred2 = np.append(total_pred2, pred2, axis=0)\n",
    "        # cost\n",
    "        total_cost = total_cost + cost\n",
    "            \n",
    "total_cost = total_cost/(TOTAL_SIZE)\n",
    "print 'The cost of model: ', total_cost\n",
    "MSE = np.concatenate((MSE, [total_cost]), axis=0)\n",
    "total_pred1 = total_pred1[1:, :]\n",
    "total_pred2 = total_pred2[1:, :]\n",
    "total_pred1 = np.transpose(total_pred1)\n",
    "total_pred2 = np.transpose(total_pred2)\n",
    "            \n",
    "print('----------End----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('../Result/'+data+'/SNR='+str(SNR)+'/Test2/LSTM'+'_d='+str(HIDDEN_SIZE)+\"_sp_\"+str(NUM_SPEAKER)+'_prediction1.csv', total_pred1, delimiter=\",\")\n",
    "# np.savetxt('../Result/'+data+'/SNR='+str(SNR)+'/Test2/LSTM'+'_d='+str(HIDDEN_SIZE)+\"_sp_\"+str(NUM_SPEAKER)+'_prediction2.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('../Result/'+data+'/SNR='+str(SNR)+'/Test2/MRNN'+'_d='+str(HIDDEN_SIZE)+'_K='+str(K)+\"_sp_\"+str(NUM_SPEAKER)+'_prediction1.csv', total_pred1, delimiter=\",\")\n",
    "np.savetxt('../Result/'+data+'/SNR='+str(SNR)+'/Test2/MRNN'+'_d='+str(HIDDEN_SIZE)+'_K='+str(K)+\"_sp_\"+str(NUM_SPEAKER)+'_prediction2.csv', total_pred1, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2-virtual",
   "language": "python",
   "name": "python2-virtual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
